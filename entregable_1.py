# -*- coding: utf-8 -*-
"""Entregable 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q_SQLP8via_K2zl9zBuYD4gZBnn21hvY
"""

#Instalación de librerías
!pip install transformers[torch]
!pip install accelerate
!pip install pandas spacy openpyxl
!pip install nltk==3.8.1
!python -m spacy download es_core_news_sm
!pip install tqdm

#Permisos de acceso a Google Drive para tomar las historias clínicas y poner el dataset final
from google.colab import drive
drive.mount('/content/drive/')

"""**1. Preprocesamiento de los datos**"""

import os
import pandas as pd

#Ruta a la carpeta que contiene los .txt
carpeta = "/content/drive/MyDrive/tareas_analitica_de_datos_en_salud/Tarea 2/Notas_Cancer_Mama"

#Lista para almacenar los registros
registros = []

#Recorrido por todas loas hictorias clínicas .txt
for archivo_nombre in sorted(os.listdir(carpeta)):
    if archivo_nombre.endswith(".txt"):
        id_historia = archivo_nombre.replace(".txt", "")
        ruta_archivo = os.path.join(carpeta, archivo_nombre)

        with open(ruta_archivo, "r", encoding="utf-8") as archivo:
            observacion = archivo.read().strip()
            registros.append({
                "id_historia": id_historia,
                "observacion": observacion
            })

#Creación del dataframe
df = pd.DataFrame(registros)

#Verificación del dataframe
df.head()

"""**2. Limpieza y tokenización por oración**"""

#Importar librerías
import pandas as pd
import nltk
import re
import spacy
import string
from nltk.corpus import stopwords

#Descargar tokenizador y stopwords
nltk.download('punkt')
nltk.download('stopwords')

#Cargar modelos
from nltk.tokenize import sent_tokenize
nlp = spacy.load("es_core_news_sm")
stop_words = set(stopwords.words("spanish"))

#Definir función para tokenizar oraciones
def tokenizar_por_oracion_nltk(texto):
    if pd.isna(texto):
        return []
    return sent_tokenize(texto, language="spanish")

#Tokenizar el dataframe
df["oraciones_nltk"] = df["observacion"].astype(str).apply(tokenizar_por_oracion_nltk)
df_exploded = df.explode("oraciones_nltk").dropna(subset=["oraciones_nltk"]).reset_index(drop=True) #Convierte c/d oración en una nueva fila

#Definir función para limpiar texto
def limpiar_texto(texto):
    if pd.isna(texto):
        return ""
    texto = re.sub(r'[\!\'\?\¿\¡\«\»\*\(\)\"\;]', '', texto)
    return texto.strip()

#Limpiar el dataset
df_exploded["oraciones"] = df_exploded["oraciones_nltk"].apply(limpiar_texto)
df_exploded = df_exploded[df_exploded["oraciones"].str.strip() != ""]
df_exploded = df_exploded.drop_duplicates(subset=["oraciones"]) #Eliminar oraciones duplicadas

df_exploded["oracion_id"] = range(1, len(df_exploded) + 1) #Asignar ID a c/d oración

df_final = df_exploded[["id_historia", "oracion_id", "oraciones"]]

#Verificación del dataframe
df_final.head()

"""**3. Extracción de Entidades**

**breast-cancer-biomedical-ner-sp**

Versión ajustada del modelo FacebookAI/xlm-roberta-large con un dataset de cáncer de mama en español. Desempeño del modelo:


**Loss:** 0.0178

**Precision:** 0.9793

**Recall:** 0.9867

**F1:** 0.9830

**Accuracy:** 0.9948
"""

#Importar librerías
import pandas as pd
from transformers import pipeline

#Cargar modelo NER
pipe = pipeline("token-classification", model="anvorja/breast-cancer-biomedical-ner-sp-1")

df = df_final

columna_texto = "oraciones"

resultados_lista = []

#Procesar cada oración con el modelo
for texto in df[columna_texto].dropna():
    resultados = pipe(texto)

    for resultado in resultados:
        resultados_lista.append({
            "Texto": texto,
            "Palabra": resultado['word'],
            "Entidad": resultado['entity'],
            "Score": resultado['score']
        })

#Creación del dataframe
df_resultados = pd.DataFrame(resultados_lista)

#Inclusión de las columnas originales en el dataframe
df_final = df.merge(df_resultados, left_on=columna_texto, right_on="Texto", how="inner")

df_final.drop(columns=["Texto"], inplace=True)

df_final.to_excel("/content/drive/MyDrive/tareas_analitica_de_datos_en_salud/Tarea 2/entidades_extraidas_token.xlsx", index=False, engine='openpyxl')

#Verificación del dataframe
df_final.head()

"""**4. Validación de la Extracción de Entidades**"""

#Score promedio por entidad
df_final['Entidad_sin_prefijo'] = df_final['Entidad'].str.replace(r'^[B|I]-', '', regex=True)

promedios_por_entidad = df_final.groupby("Entidad_sin_prefijo")["Score"].mean().reset_index()

promedios_por_entidad.rename(columns={"Score": "Score_Promedio"}, inplace=True)

print(promedios_por_entidad)

#Participación por entidad en el dataset
conteo_por_entidad = df_final.groupby("Entidad").size().reset_index(name="Conteo")

total_registros = len(df_final)
conteo_por_entidad["Porcentaje"] = (conteo_por_entidad["Conteo"] / total_registros) * 100

print(conteo_por_entidad)

"""**5. Reconstrucción de las Entidades Extraídas**

**breast-cancer-biomedical-ner-sp**

Versión ajustada del modelo FacebookAI/xlm-roberta-large con un dataset de cáncer de mama en español. Desempeño del modelo:


**Loss:** 0.0178

**Precision:** 0.9793

**Recall:** 0.9867

**F1:** 0.9830

**Accuracy:** 0.9948
"""

#Importar librerías
import pandas as pd
from transformers import AutoTokenizer, AutoModelForTokenClassification
import torch
import torch.nn.functional as F
from tqdm import tqdm

#Diccionario de etiquetas
id2label = {
    0: "B-AGE", 1: "B-STAGE", 2: "B-DATE", 3: "B-IMPLICIT_DATE", 4: "B-TNM",
    5: "B-FAMILY", 6: "B-OCURRENCE_EVENT", 7: "B-TOXIC_HABITS", 8: "B-HABIT-QUANTITY",
    9: "B-TREATMENT_NAME", 10: "B-LINE_CICLE_NUMBER", 11: "B-SURGERY", 12: "B-DRUG",
    13: "B-DOSE", 14: "B-FREQ", 15: "B-BIOMARKER", 16: "B-CLINICAL_SERVICE",
    17: "B-COMORBIDITY", 18: "B-PROGRESION", 19: "B-GINECOLOGICAL_HISTORY",
    20: "B-GINE_OBSTETRICS", 21: "B-ALLERGIES", 22: "B-DURATION",
    23: "I-AGE", 24: "I-STAGE", 25: "I-DATE", 26: "I-IMPLICIT_DATE",
    27: "I-TNM", 28: "I-FAMILY", 29: "I-OCURRENCE_EVENT", 30: "I-TOXIC_HABITS",
    31: "I-HABIT-QUANTITY", 32: "I-TREATMENT_NAME", 33: "I-LINE_CICLE_NUMBER",
    34: "I-SURGERY", 35: "I-DRUG", 36: "I-DOSE", 37: "I-FREQ", 38: "I-BIOMARKER",
    39: "I-CLINICAL_SERVICE", 40: "I-COMORBIDITY", 41: "I-PROGRESION",
    42: "I-GINECOLOGICAL_HISTORY", 43: "I-GINE_OBSTETRICS", 44: "I-ALLERGIES",
    45: "I-DURATION", 46: "B-CANCER_CONCEPT", 47: "I-CANCER_CONCEPT", 48: "O"
}

num_labels = len(id2label)

#Cargar modelo y tokenizador
model = AutoModelForTokenClassification.from_pretrained(
    "anvorja/breast-cancer-biomedical-ner-sp-1",
    num_labels=num_labels,
    id2label=id2label,
    label2id={v: k for k, v in id2label.items()}
)
tokenizer = AutoTokenizer.from_pretrained("anvorja/breast-cancer-biomedical-ner-sp-1", use_fast=True)


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

df = df_final

#Procesamiento por lotes
all_results = []
batch_size = 32

#Bucle de procesamiento
for start_idx in tqdm(range(0, len(df), batch_size), desc="Procesando lotes"):
    end_idx = min(start_idx + batch_size, len(df))
    batch = df.iloc[start_idx:end_idx]
    texts = batch["oraciones"].tolist()

    #Tokenización
    encodings = tokenizer(
        texts,
        truncation=True,
        padding=True,
        return_offsets_mapping=True,
        return_attention_mask=True,
        return_token_type_ids=False,
        max_length=512,
        is_split_into_words=False
    )

    input_ids = torch.tensor(encodings["input_ids"]).to(device)
    attention_mask = torch.tensor(encodings["attention_mask"]).to(device)

    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    probs = F.softmax(logits, dim=-1)

    #Reconstruir las palabras tokenizadas por el tokenizador
    for i, text in enumerate(texts):
        word_ids = encodings.word_ids(batch_index=i)
        tokens = tokenizer.convert_ids_to_tokens(encodings["input_ids"][i])

        previous_word_id = None
        aligned_words, aligned_labels, aligned_scores = [], [], []

        for token, label_id, word_id in zip(tokens, predictions[i].tolist(), word_ids):
            if word_id is None:
                continue
            if word_id != previous_word_id:
                aligned_words.append(token.replace("▁", ""))
                aligned_labels.append(id2label[label_id])
                aligned_scores.append(probs[i][word_id][label_id].item())
            else:
                aligned_words[-1] += token.replace("▁", "")
            previous_word_id = word_id

        #Se toman las entidades diferentes de "O"
        filtered_results = [
            (word, label, score)
            for word, label, score in zip(aligned_words, aligned_labels, aligned_scores)
            if label != "O"
        ]

        #Combinar tokens consecutivos de una misma entidad
        combined_results = []
        temp_entity, temp_label, temp_score = "", "", 0
        for word, label, score in filtered_results:
            if label.startswith("B-"):
                if temp_entity:
                    combined_results.append((temp_entity, temp_label, temp_score))
                temp_entity, temp_label, temp_score = word, label, score
            elif label.startswith("I-") and label[2:] == temp_label[2:]:
                temp_entity += " " + word
                temp_score += score
            else:
                if temp_entity:
                    combined_results.append((temp_entity, temp_label, temp_score))
                temp_entity, temp_label, temp_score = word, label, score

        if temp_entity:
            combined_results.append((temp_entity, temp_label, temp_score))

        #Guardar resultados por oración
        for entity, label, score in combined_results:
            result = batch.iloc[i].to_dict()
            result["Palabra"] = entity
            result["Entidad"] = label[2:]
            result["oracion_id"] = batch.iloc[i]["oracion_id"]
            all_results.append(result)

#Guardar resultados en dataframe
results_df = pd.DataFrame(all_results)
results_df

results_df.to_excel("/content/drive/MyDrive/tareas_analitica_de_datos_en_salud/Tarea 2/entidades_extraidas.xlsx", index=False, engine='openpyxl') #Enviar dataframe a Google Drive