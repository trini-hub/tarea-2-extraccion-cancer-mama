# -*- coding: utf-8 -*-
"""Entregable 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nciI67Q48t32RKtcMJCvM1ddrM0_OqDk
"""

#Instalación de librerías
!pip install transformers torch
!pip install accelerate
!pip install pandas openpyxl
!pip install tqdm
!python -m spacy download es_core_news_sm
!pip install nltk==3.8.1

#Permisos de acceso a Google Drive para tomar las historias clínicas y poner el dataset final
from google.colab import drive
drive.mount('/content/drive/')

"""**1. Preprocesamiento de los datos**"""

import os
import pandas as pd

#Ruta a la carpeta que contiene los .txt
carpeta = "/content/drive/MyDrive/tareas_analitica_de_datos_en_salud/Tarea 2/Notas_Cancer_Mama"

#Lista para almacenar los registros
registros = []

#Recorrido por todas loas hictorias clínicas .txt
for archivo_nombre in sorted(os.listdir(carpeta)):
    if archivo_nombre.endswith(".txt"):
        id_historia = archivo_nombre.replace(".txt", "")
        ruta_archivo = os.path.join(carpeta, archivo_nombre)

        with open(ruta_archivo, "r", encoding="utf-8") as archivo:
            observacion = archivo.read().strip()
            registros.append({
                "id_historia": id_historia,
                "observacion": observacion
            })

#Creación del dataframe
df = pd.DataFrame(registros)

#Verificación del dataframe
df.head()

"""**2. Limpieza y tokenización por oración**"""

#Importar librerías
import pandas as pd
import nltk
import re
import spacy
import string
from nltk.corpus import stopwords

#Descargar tokenizador y stopwords
nltk.download('punkt')
nltk.download('stopwords')

#Cargar modelos
from nltk.tokenize import sent_tokenize
nlp = spacy.load("es_core_news_sm")
stop_words = set(stopwords.words("spanish"))

#Definir función para tokenizar oraciones
def tokenizar_por_oracion_nltk(texto):
    if pd.isna(texto):
        return []
    return sent_tokenize(texto, language="spanish")

#Tokenizar el dataframe
df["oraciones_nltk"] = df["observacion"].astype(str).apply(tokenizar_por_oracion_nltk)
df_exploded = df.explode("oraciones_nltk").dropna(subset=["oraciones_nltk"]).reset_index(drop=True) #Convierte c/d oración en una nueva fila

#Definir función para limpiar texto
def limpiar_texto(texto):
    if pd.isna(texto):
        return ""
    texto = re.sub(r'[\!\'\?\¿\¡\«\»\*\(\)\"\;]', '', texto)
    return texto.strip()

#Limpiar el dataset
df_exploded["oraciones"] = df_exploded["oraciones_nltk"].apply(limpiar_texto)
df_exploded = df_exploded[df_exploded["oraciones"].str.strip() != ""]
df_exploded = df_exploded.drop_duplicates(subset=["oraciones"]) #Eliminar oraciones duplicadas

df_exploded["oracion_id"] = range(1, len(df_exploded) + 1) #Asignar ID a c/d oración

df_final = df_exploded[["id_historia", "oracion_id", "oraciones"]]

#Verificación del dataframe
df_final.head()

"""**3. Extracción de Negación e Incertidumbre**

**bert-base-uncased-finetuned-ner-negation_detection_NUBES**

Versión ajustada del modelo bert-base-uncased con el corpus NUBES. Desempeño del modelo:

**Loss:** 0.1662

**Precision:** 0.8435

**Recall:** 0.8703

**F1:** 0.8567

**Accuracy:** 0.9632
"""

#Importar librerías
import pandas as pd
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
from tqdm import tqdm

#Cargar modelo y tokenizador
modelo_negacion = "JuanSolarte99/bert-base-uncased-finetuned-ner-negation_detection_NUBES"
tokenizer = AutoTokenizer.from_pretrained(modelo_negacion)
model = AutoModelForTokenClassification.from_pretrained(modelo_negacion)

#Asignar etiquetas al modelo
model.config.id2label = {
    0: "B-NEG",
    1: "B-NSCO",
    2: "B-UNC",
    3: "B-USCO",
    4: "I-NEG",
    5: "I-NSCO",
    6: "I-UNC",
    7: "I-USCO",
    8: "O"
}

model.config.label2id = {v: k for k, v in model.config.id2label.items()}

#Crear pipeline
pipeline_negacion = pipeline("ner", model=model, tokenizer=tokenizer)

df = df_final


tqdm.pandas()
resultados = []

#Procesar cada oración y aplicar el modelo
for _, fila in tqdm(df.iterrows(), total=len(df), desc="Procesando observaciones"):
    id_historia = fila["id_historia"]
    texto = fila["oraciones"]

    try:
        entidades = pipeline_negacion(texto)
    except Exception as e:
        print(f"Error al procesar ID {id_historia}: {e}")
        continue

    #Guardar cada entidad con su tipo (negada, afirmada o especulada)
    for ent in entidades:
        resultados.append({
            "id_historia": id_historia,
            "texto_original": texto,
            "entidad": ent["word"],
            "etiqueta": ent["entity"],
            "score": round(ent["score"], 3)
        })


df_resultado = pd.DataFrame(resultados)

ruta_salida = "/content/drive/MyDrive/tareas_analitica_de_datos_en_salud/Tarea 2/entidades_extraidas_negacion_token.xlsx"
df_resultado.to_excel(ruta_salida, index=False)

#Verificación del dataframe
df_resultado.head()

"""4. Validación de la Extracción de Negación e Incertidumbre"""

#Score promedio por etiqueta
df_resultado['etiqueta'] = df_resultado['etiqueta'].str.replace(r'^[B|I]-', '', regex=True)

promedios_por_etiqueta = df_resultado.groupby("etiqueta")["score"].mean().reset_index()

promedios_por_etiqueta.rename(columns={"score": "score_Promedio"}, inplace=True)

print(promedios_por_etiqueta)

#Participación por etiqueta en el dataset
conteo_por_etiqueta = df_resultado.groupby("etiqueta").size().reset_index(name="Conteo")

total_registros = len(df_resultado)
conteo_por_etiqueta["Porcentaje"] = (conteo_por_etiqueta["Conteo"] / total_registros) * 100

print(conteo_por_etiqueta)

"""4. Reconstrucción de las Entidades Extraídas"""

#Importar librerías
import pandas as pd
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
from tqdm import tqdm

#Cargar modelo y tokenizador
modelo_negacion = "JuanSolarte99/bert-base-uncased-finetuned-ner-negation_detection_NUBES"
tokenizer = AutoTokenizer.from_pretrained(modelo_negacion)
model = AutoModelForTokenClassification.from_pretrained(modelo_negacion)

#Asignar etiquetas al modelo
model.config.id2label = {
    0: "B-NEG",
    1: "B-NSCO",
    2: "B-UNC",
    3: "B-USCO",
    4: "I-NEG",
    5: "I-NSCO",
    6: "I-UNC",
    7: "I-USCO",
    8: "O"
}
model.config.label2id = {v: k for k, v in model.config.id2label.items()}

#Pipeline con estrategia de agregación
pipeline_negacion = pipeline("ner", model=model, tokenizer=tokenizer, aggregation_strategy="simple")

#Definición de función para clasificar el estado lógico de las etiquetas
def clasificar_estado(etiqueta):
    if etiqueta in ["B-NEG", "I-NEG", "NEG", "NSCO", "B-NSCO", "I-NSCO"]:
        return "negado"
    elif etiqueta in ["B-UNC", "I-UNC", "UNC", "USCO", "B-USCO", "I-USCO"]:
        return "incierto"
    else:
        return "afirmado"

df = df_final

tqdm.pandas()
resultados = []

#Aplicar modelo y función definida
for _, fila in tqdm(df.iterrows(), total=len(df), desc="Procesando observaciones"):
    id_historia = fila["id_historia"]
    texto = fila["oraciones"]

    try:
        entidades = pipeline_negacion(texto)
    except Exception as e:
        print(f"⚠️ Error al procesar ID {id_historia}: {e}")
        continue

    for ent in entidades:
        resultados.append({
            "id_historia": id_historia,
            "texto_original": texto,
            "entidad": ent["word"],
            "etiqueta": ent["entity_group"],
            "estado": clasificar_estado(ent["entity_group"]),
            "score": round(ent["score"], 3)
        })

df_resultado2 = pd.DataFrame(resultados)


ruta_salida = "/content/drive/MyDrive/tareas_analitica_de_datos_en_salud/Tarea 2/entidades_extraidas_negacion_clasificado.xlsx"
df_resultado2.to_excel(ruta_salida, index=False)